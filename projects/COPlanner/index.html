
<head>
	<title>COPlanner</title>
	<meta property="og:title" content="COPlanner">
	<meta property="og:description" content="COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL">
	<link rel="stylesheet" href="style.css">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<script type="text/javascript">
		function toggle(id) {
			var e = document.getElementById(id);
			if(e.style.display == 'block')
				e.style.display = 'none';
			else
				e.style.display = 'block';
		}
	</script>
</head>
<!-- <head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL</title>
<meta name="description" content="Abstract">
<link rel="stylesheet" href="asset/css/drm.css">
<link rel="canonical" href="https://si0wang.github.io/">
<link rel="shortcut icon" href="asset/favicon.jpg"> 
</head> -->
<div class="header" id="top">
	<div style="padding-bottom: 32px; text-align: center;">
		<a href="https://si0wang.github.io/"><i class="fa fa-home"></i>&ensp;More research</a>
	</div>
	<h1><span class="bold">COPlanner: Plan to Roll Out Conservatively<br/> but to Explore Optimistically for Model-Based RL</h1>
	<table class="authors">
		<tbody>
			<tr>
				<td>
					<h4>
						<a href="https://si0wang.github.io/" class="nobreak">Xiyao Wang</a><sup>1</sup>,&ensp;
						<a href="https://ruijiezheng.com/" class="nobreak">Ruijie Zhang</a><sup>1</sup>,&ensp;
						<a href="https://ycsun2017.github.io/" class="nobreak">Yanchao Sun</a><sup>2</sup>,&ensp;
            <a href="https://jiaruonan.github.io/" class="nobreak">Ruonan Jia</a><sup>3</sup>,&ensp;
            <a href="https://wwongkamjan.github.io/" class="nobreak">Wichayaporn Wongkamjan</a><sup>1</sup>,&ensp;
            <a href="http://hxu.rocks/" class="nobreak">Huazhe Xu</a><sup>2</sup>,&ensp;
            <a href="https://furong-huang.com/" class="nobreak">Furong Huang</a><sup>1</sup>,&ensp;
            <br/>
						<span class="authors-affiliation"><sup>1</sup> University of Maryland, College Park</span>
            <span class="authors-affiliation">&ensp;<sup>2</sup> JPMorgan AI Research</span>
						<span class="authors-affiliation"><sup>3</sup> Tsinghua University</span>
					</h4>
				</td>
			</tr>
		</tbody>
	</table>
	<div class="links">
		<a href="https://arxiv.org/abs/2310.07220" class="btn"><i class="fa">&#xf1c1;</i>&ensp;Paper</a><a href="https://github.com/si0wang/COPlanner" class="btn"><i class="fa fa-github"></i>&ensp;Code</a>
	</div>
</div>
<div class="content">
	<div class="hr"></div>
	<div class="figure" style="height: 320px; background-image: url(Figures/framework_v3.png);"></div>
	<div style="margin: auto; margin-top: -24px;">
		<p>
		We present <span class="bold red">COPlanner</span>, a powerful planning-driven framework for model-based Reinforcement Learning to mitigate the impact of imperfect world models and boosting policy learning. <span class="bold red">COPlanner</span> is a plug-and-play framework that can be applied to any dyna-style model-based methods.		
		</p>
	</div>
	<div class="hr"></div>
	<div>
		<h2>Abstract</h2>
		<p class="abstract">
Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose COPlanner, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. COPlanner leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration  respectively, to choose actions. Consequently, COPlanner can avoid model uncertain regions through conservative model rollouts,  thereby alleviating the influence of model error. Simultaneously, it explores high-reward model uncertain regions to reduce model error actively through optimistic real environment exploration. COPlanner is a plug-and-play framework that can be applied to any dyna-style model-based methods. Experimental results on a series of proprioceptive and visual continuous control tasks demonstrate that both sample efficiency and asymptotic performance of strong model-based methods are significantly improved combined with COPlanner.		</p>
	</div>
	<div class="hr"></div>
	<div>
		<h2>TD-MPC2 Learns Diverse Tasks</h2>
		<div class="figure-caption">
			<p>
				We evaluate TD-MPC<span class="bold">2</span> on <span class="bold">104</span> control tasks across 4 task domains: <a href="https://arxiv.org/abs/1801.00690">DMControl</a>, <a href="https://meta-world.github.io">Meta-World</a>, <a href="https://maniskill2.github.io">ManiSkill2</a>, and <a href="https://sites.google.com/view/myosuite">MyoSuite</a>.
			</p>
		
	</div>
	<div class="hr"></div>
	<div>
		<h2>Comparison with end-to-end world model-based methods</h2>
		<div class="figure-caption">
			TD-MPC<span class="bold">2</span> compares favorably to existing model-free (<a href="https://arxiv.org/abs/1812.05905">SAC</a>) and model-based (<a href="https://arxiv.org/abs/2301.04104">DreamerV3</a> and <a href="https://arxiv.org/abs/2203.04955">TD-MPC</a>) online RL methods on a diverse set of 104 continuous control tasks. By design, TD-MPC2 achieves consistently strong results without <span class="italic">any</span> hyperparameter tuning, which has been instrumental for scaling to large massively multi-task world models.
		</div>
		<div class="figure" style="height: 208px; background-image: url(Figures/exp_propinput_new.png);"></div>
	</div>
	<div class="hr"></div>
	<div>
		<h2>Comparison with latent world model-based methods</h2>
		<div class="figure-caption">
			We evaluate the performance of 5 multitask models ranging from 1M to 317M parameters on a collection of 80 diverse tasks that span multiple task domains and vary greatly in objective, embodiment, and action space. The task set consists of all 50 Meta-World tasks, as well as 30 DMControl tasks. We also report scaling results on the DMControl subset. We observe that agent capabilities consistently increase with model size on both task sets.
		</div>
		<div class="figure" style="height: 288px; background-image: url(Figures/exp_dreamerv3_new.png);"></div>
		<div class="figure-caption">
			We evaluate the performance of 5 multitask models ranging from 1M to 317M parameters on a collection of 80 diverse tasks that span multiple task domains and vary greatly in objective, embodiment, and action space. The task set consists of all 50 Meta-World tasks, as well as 30 DMControl tasks. We also report scaling results on the DMControl subset. We observe that agent capabilities consistently increase with model size on both task sets.
		</div>
		<div class="figure" style="height: 288px; background-image: url(Figures/exp_propdreamer.png);"></div>
	</div>
	<div class="hr"></div>
	<div>
		<h2>Supporting Open-Source Science</h2>
		<div class="figure-caption">
			<p>
				We open-source a total of <span class="bold">324</span> TD-MPC<span class="bold">2</span> model checkpoints, including <span class="bold">12</span> multi-task models (ranging from 1M to 317M parameters) trained on 80, 70, and 30 tasks, respectively.
			</p>
			<div class="links" style="margin-bottom: 28px;">
				<a href="/td-mpc2/models" class="btn"><i class="fa fa-cogs"></i>&ensp;Models</a><a href="/td-mpc2/dataset" class="btn"><i class="fa fa-database"></i>&ensp;Dataset</a>
			</div>
			<p>
				Additionally, we also release the two 545M and 345M transition datasets that we used to train our multi-task models. The datasets are sourced from the replay buffers of 240 single-task agents and thus contain a wide range of behaviors.<br/><br/>
			</p>
			<table class="models">
				<tbody>
					<tr>
						<th>Domains</th>
						<th>Tasks</th>
						<th>Embodiments</th>
						<th>Episodes</th>
						<th>Transitions</th>
						<th>Size</th>
						<th>Link</th>
					</tr>
					<tr class="models">
						<td>
							<video playsinline="" autoplay="" loop="" preload="" muted="" height="40px">
								<source src="videos/tdmpc2-walker-run.mp4" type="video/mp4"/>
							</video>
							<video playsinline="" autoplay="" loop="" preload="" muted="" height="40px">
								<source src="videos/tdmpc2-assembly.mp4" type="video/mp4"/>
							</video>
							<br/>
							DMControl + Meta-World
						</td>
						<td>
							80
						</td>
						<td>
							12
						</td>
						<td>
							2.69M
						</td>
						<td>
							545M
						</td>
						<td>
							34GB
						</td>
						<td>
							<a href="/td-mpc2/dataset">Download</a>
						</td>
					</tr>
					<tr class="models">
						<td>
							<video playsinline="" autoplay="" loop="" preload="" muted="" height="40px">
								<source src="videos/tdmpc2-walker-run.mp4" type="video/mp4"/>
							</video>
							<br/>
							DMControl
						</td>
						<td>
							30
						</td>
						<td>
							11
						</td>
						<td>
							690k
						</td>
						<td>
							345M
						</td>
						<td>
							20GB
						</td>
						<td>
							<a href="/td-mpc2/dataset">Download</a>
						</td>
					</tr>
				</tbody>
			</table>
			<p>
				We are excited to see what the community will do with these models and datasets, and hope that our release will encourage other research labs to open-source their checkpoints as well.
			</p>
		</div>
	</div>
	<div class="hr"></div>
	<div style="margin: auto; margin-bottom: 64px; text-align: center;">
		<h2>Paper</h2>
		<span class="bold">TD-MPC<span class="vbold red">2</span>: Scalable, Robust World Models for Continuous Control</span><br/>
		<span class="italic">Nicklas Hansen, Hao Su, Xiaolong Wang</span><br/><br/>
		<a href="https://arxiv.org/abs/2310.16828">arXiv preprint</a><br/><br/>
		<div class="page" style="background-image: url(thumbnails/0.png);"></div>
		<div class="page" style="background-image: url(thumbnails/1.png);"></div>
		<div class="page" style="background-image: url(thumbnails/2.png);"></div>
		<div class="page" style="background-image: url(thumbnails/3.png);"></div>
		<div class="page" style="background-image: url(thumbnails/4.png);"></div>
		<div class="page" style="background-image: url(thumbnails/5.png);"></div>
		<div class="page" style="background-image: url(thumbnails/6.png);"></div>
		<div class="page" style="background-image: url(thumbnails/7.png);"></div>
		<div class="page" style="background-image: url(thumbnails/8.png);"></div>
		<div style="margin: auto; margin-top: 32px;">
			<a href="https://arxiv.org/abs/2310.16828">View on arXiv</a>
		</div>
	</div>
	<div class="hr"></div>
	<div style="padding-bottom: 64px; text-align: center;">
		<h2>Citation</h2>
		<p>
			If you find our work useful, please consider citing the paper as follows:
		</p>
		<div id="bibtex-text" class="bibtexsection" onClick="window.getSelection().selectAllChildren(document.getElementById('bibtex-text'));">
@misc{hansen2023tdmpc2,
	title={TD-MPC2: Scalable, Robust World Models for Continuous Control}, 
	author={Nicklas Hansen and Hao Su and Xiaolong Wang},
	year={2023},
	eprint={2310.16828},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}</div>
	</div>
</div>
<footer>
<a href="#top"><i class="fa fa-arrow-up"></i><br/>Return to top</a>
<div style="padding-top: 48px;">
	<span>Website based on <a href="https://nicklashansen.github.io/td-mpc">TD-MPC</a> and <a href="https://nerfies.github.io">Nerfies</a>.</span>
</div>
</footer>
