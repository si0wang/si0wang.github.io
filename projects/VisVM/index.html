<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VisVM</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
            </h1>
            <h4 class="title is-2 publication-title">Scaling Inference-Time Search with Vision Value Model 
		    for Improved Visual Comprehension</h4>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <a href="https://si0wang.github.io/">Xiyao Wang</a><sup>1, 2</sup>,</span>
              <span class="author-block">
              <a href="https://zyang-ur.github.io/">Zhengyuan Yang</a><sup>2</sup>,</span>
              <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/linjli/">Linjie Li</a><sup>2</sup>,</span>
              <span class="author-block">
              <a href="THIRD AUTHOR PERSONAL LINK">Hongjin Lu</a><sup>1</sup>,</span>
              <span class="author-block">
              <a href="https://yuancheng-xu.github.io/">Yuancheng Xu</a><sup>1</sup>,</span>
              <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/chunglin/">Chung-Ching Lin</a><sup>2</sup>,</span>
              <span class="author-block">
              <a href="https://sites.google.com/site/kevinlin311tw/">Kevin Lin</a><sup>2</sup>,</span>
              <span class="author-block">
              <a href="https://furong-huang.com/">Furong Huang</a><sup>1, *</sup>,</span>
              <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/lijuanw/">Lijuan Wang</a><sup>2, *</sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> University of Maryland, College Park</span><br/>
                    <span class="author-block"><sup>2</sup> Microsoft</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Advising</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2412.03704.pdf"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                      <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.03704"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/si0wang/VisVM"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/russwang/VisVM-LLaVA-Next-Mistral-7B"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                  </span>
                  <span>Model</span>
                </a>
              </span>
                      
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Despite significant advancements in vision-language
models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the
self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM)
that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically,
VisVM not only evaluates the generated sentence quality in
the current search step, but also anticipates the quality of
subsequent sentences that may result from the current step,
thus providing a long-term value. In this way, VisVM steers
VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality 
responses. Experimental results demonstrate that VisVMguided search significantly enhances VLMs‚Äô ability to generate descriptive captions with richer visual details and
fewer hallucinations, compared with greedy decoding and
search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVMguided captions improve VLM‚Äôs performance across a wide
range of multimodal benchmarks, indicating the potential
for developing self-improving VLMs.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3" id="Vision Value Model">Vision Value Model</h2>
        <div class="content has-text-justified">
          <p > <b>Vis</b>ion <b>V</b>alue <b>M</b>odel (VisVM) is a value network which can provide reward signal to guide VLM inference-time
search by generating descriptive captions in a step-by-step manner. 
	  </p>
		</div>
		<div class="content has-text-centered">
                <img src="static/images/figure1.png" width="1000" height="600"/>
          </div> 
	<div class="content has-text-justified">
	<p>
		The core innovation lies in breaking down diverse responses from VLM into sentence pairs and using CLIP as the reward signal to train VisVM through <b>Temporal Difference learning</b>.
	</p>
	  </div>  
	      <div class="content has-text-centered">
                <img src="static/images/TD learning.png" width="600" height="75"/>
          </div>
	<div class="content has-text-justified">
	<p>
This enables VisVM to predict the impact of current sentences on future generations, 
allowing it to avoid response candidates during inference time
with higher hallucination risks and generate image descriptions that are less prone to hallucination and more detailed.
	</p>
  
      </div>
    </div>
  
  </div>
	  </div>
</section>



<section class="section">
  <div class="container">
  <div class="columns is-centered">
    <div class="column is-full-width has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
<!--         <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Inference-time Search using VisVM</h2>
        <div class="content has-text-justified">
            <p>We use VisVM as the signal to guide the
VLM inference-time search for generating higher-quality
responses. At each searching step we sample several sentence candidates and evaluate the value using VisVM. The candidate with the highest value is selected as the
response for the current step. This process continues iteratively until the complete response sequence is generated.
            </p>
          <div class="content has-text-centered">
                <img src="static/images/search.png" width="500" height="600"/>
          </div> 
        </div>
<!--         </div> -->
      </div>
    </div>
 </div>
  </section>
  
<section class="section">
  <div class="container">
<div class="columns is-centered">
    <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">VisVM-Guided Search Improves Response Quality</h2>
        <div class="content has-text-justified">
	<ul>
            <li> <b>Response Quality Evaluation</b>
            We use Human and GPT-4o as judges to compare image descriptions generated by VisVM Guided Search, CLIP-PRM Guided Search, Best of N, and greedy decoding. 
		    We select LLaVA-Next-Mistral-7B as base model for generating responses.
		    The results show that descriptions from VisVM Guided Search are significantly more preferred.
	    </li>
          <div class="content has-text-centered">
		<img src="static/images/human_eval.png" width="600" height="300"/>
                <img src="static/images/gpt_eval.png" width="600" height="600"/>
          </div> 
		<li><b>Less Hallucination, More Details</b>
            		The image description obtained using VisVM search not only significantly reduces hallucinations, 
			but also provides a more precise and detailed depiction of the image. 
			For example, in this case, the description includes subtle details that even a meticulous human annotator might overlook, 
			such as ‚ÄúThere are also green street signs...which are partially obscured by the raindrops on the windshield.‚Äù
		</li>
	<div class="content has-text-centered">
                <img src="static/images/case1.png" width="1000" height="1000"/>
          </div> 
		<li><b>Improved Hallucination Benchmark Performance</b>
            By incorporating VisVM guided search during inference, LLaVA-Next achieves remarkable improvements on the hallucination benchmarks, 
			significantly outperforming other search/decoding baselines.
		</li>
	<div class="content has-text-centered">
                <img src="static/images/inference.png" width="800" height="400"/>
          </div> 
	</ul>
        </div>
<!--         </div> -->
      </div>
    </div>
</div>
  </section>


<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">VisVM for Self-Training Vision-Language Model</h2>
	<div class="content has-text-justified">
	<p>
	We use LLAVA-next-7B as the base model, leveraging VisVM as a reward signal to generate high-quality image descriptions as SFT data and train LLaVA-Next-7B. 
	Across nine comprehension and hallucination benchmarks, VisVM-guided self-training boosted LLAVA-next-7B's performance by an average of 10.8%. 
	This demonstrates the potential of applying this method for a genuine self-training pipeline that continuously enhances visual comprehension.
	</p>
	  </div>  
	      <div class="content has-text-centered">
                <img src="static/images/sfttable.png" width="1000" height="400"/>
          </div>
	</div>  
	      <div class="content has-text-centered">
                <img src="static/images/sftfig.png" width="600" height="600"/>
          </div>
    </div>
  
  </div>
	  </div>
</section>
	
<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <p>If you find our work useful, please consider citing the paper as follows:</p>
      <pre><code>
@misc{wang2024scalinginferencetimesearchvision,
      title={Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension}, 
      author={Xiyao Wang and Zhengyuan Yang and Linjie Li and Hongjin Lu and Yuancheng Xu and Chung-Ching Lin and Kevin Lin and Furong Huang and Lijuan Wang},
      year={2024},
      eprint={2412.03704},
      archivePrefix={arXiv},
      primaryClass={cs.CV}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
