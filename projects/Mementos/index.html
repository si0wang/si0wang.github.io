<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Mementos</title>
  <link rel="icon" type="image/x-icon" href="static/images/film_frames.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/film_frames.png" style="width:1em;vertical-align: middle" alt="Logo"/>
            <span>Mementos</span>
            </h1>
            <h4 class="title is-3 publication-title">A Comprehensive Benchmark for Multimodal Large Language
              Model Reasoning over Image Sequences</h4>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <a href="https://si0wang.github.io/">Xiyao Wang</a><sup>1</sup>,</span>
              <span class="author-block">
              <a href="https://tonyzhou98.github.io/">Yuhang Zhou</a><sup>1</sup>,</span>
              <span class="author-block">
              <a href="https://www.linkedin.com/in/xiaoyuliu1231/">Xiaoyu Liu</a><sup>1</sup>,</span>
              <span class="author-block">
              <a href="THIRD AUTHOR PERSONAL LINK">Hongjin Lu</a><sup>1</sup>,</span>
              <span class="author-block">
              <a href="https://yuancheng-xu.github.io/">Yuancheng Xu</a><sup>1</sup>,</span>
              <span class="author-block">
              <a href="https://www.linkedin.com/in/feihong-he/">Feihong He</a><sup>2</sup>,</span>
              <span class="author-block">
              <a href="https://jaehong31.github.io/">Jaehong Yoon</a><sup>2</sup>,</span>
              <span class="author-block">
              <a href="https://www.linkedin.com/in/lu-taixi/">Taixi Lu</a><sup>2</sup>,</span>
              <span class="author-block">
              <a href="https://www.gedasbertasius.com/">Gedas Bertasius</a><sup>2</sup>,</span>
              <span class="author-block">
              <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a><sup>2</sup>,</span>
              <span class="author-block">
              <a href="https://www.huaxiuyao.io/">Huaxiu Yao</a><sup>2*</sup>,</span>
              <span class="author-block">
              <a href="https://furong-huang.com/">Furong Huang</a><sup>1*</sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> University of Maryland, College Park</span><br/>
                    <span class="author-block"><sup>2</sup> UNC-Chapel Hill, Chapel Hill</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Advising</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2401.10529.pdf"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                      <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.10529"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/si0wang/Mementos"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/si0wang/Mementos"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="https://si0wang.github.io/projects/Mementos/#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üèÜ</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
                      
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal Large Language Models (MLLMs)
have demonstrated proficiency in handling a
variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static
information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been
less investigated. To address this challenge,
this paper introduces Mementos, a new benchmark designed to assess MLLMs‚Äô sequential
image reasoning abilities. Mementos features
4,761 diverse image sequences with varying
lengths. We also employ a GPT-4 assisted
method to evaluate MLLM reasoning performance. Through a careful evaluation of nine
recent MLLMs on Mementos, including GPT4V and Gemini, we find that they struggle to
accurately describe dynamic information about
given image sequences, often leading to hallucinations/misrepresentations of objects and
their corresponding behaviors. Our quantitative analysis and case studies identify three key
factors impacting MLLMs‚Äô sequential image
reasoning: the correlation between object and
behavioral hallucinations, the influence of cooccurring behaviors, and the compounding impact of behavioral hallucinations.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard_test">Leaderboard</h2>
        <div class="content">
          <p class="mt-3">Recall. Precision, and F1 scores of Object and Behavior on <b>Val</b> set of <img src="static/images/film_frames.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">Mementos</span>.
          </p>

          <table class="js-sort-table" id="results">
            <tr>
                <td class="js-sort-number"><strong>#</strong></td>
                <td class="js-sort-number"><strong>Model</strong></td>
                <td class="js-sort-number"><strong>Input type</strong></td>
                <td class="js-sort-number"><strong>Source</strong></td>
                <td class="js-sort-number"><strong>Date</strong></td>
                <td class="js-sort-number"><strong><u>Avg</u></strong></td>
                <td class="js-sort-number"><strong>Object-Recall</strong></td>
                <td class="js-sort-number"><strong>Object-Precision</strong></td>
                <td class="js-sort-number"><strong>Object-F1</strong></td>
                <td class="js-sort-number"><strong>Behavior-Recall</strong></td>
                <td class="js-sort-number"><strong>Behavior-Precision</strong></td>
                <td class="js-sort-number"><strong>Behavior-F1</strong></td>
            </tr>
            <tr>
              <td>1</td>
              <td><b class="best-score-text">GPT-4V ü•á</b></td>
              <td>Sequential</td>
              <td><a href="https://openai.com/research/gpt-4v-system-card" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b class="best-score-text">44.33</b></td>
              <td>55.9</td>
              <td>34.7</td>
              <td>29.7</td>
              <td>58.8</td>
              <td>42.4</td>
              <td>40.7</td>                               
            </tr>
            <tr>
              <td>2</td>
              <td><b class="best-score-text">Gemini ü•à</b></td>
              <td>Sequential</td>
              <td><a href="https://arxiv.org/abs/2312.11805" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b class="best-score-text">42.68</b></td>
              <td>50.3</td>
              <td>29.7</td>
              <td>40.9</td>
              <td>49.3</td>
              <td>43.3</td>
              <td>33.9</td>                          
            </tr>
            <tr>
              <td>3</td>
              <td><b class="best-score-text">GPT-4v ü•â</b></td>
              <td>Combined</td>
              <td><a href="https://openai.com/research/gpt-4v-system-card" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b class="best-score-text">31.74</b></td>
              <td>27.6</td>
              <td>37.4</td>
              <td>23.9</td>
              <td>43.0</td>
              <td>30.3</td>
              <td>37.1</td>                               
            </tr>
            <tr>
              <td>2</td>
              <td><b class="best-score-text">Gemini</b></td>
              <td>Combined</td>
              <td><a href="https://arxiv.org/abs/2312.11805" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b class="best-score-text">42.68</b></td>
              <td>50.3</td>
              <td>29.7</td>
              <td>40.9</td>
              <td>49.3</td>
              <td>43.3</td>
              <td>33.9</td>                          
            </tr>
            <tr>
              <td>2</td>
              <td><b class="best-score-text">Video-LLaMA-2</b></td>
              <td>Sequential</td>
              <td><a href="https://github.com/DAMO-NLP-SG/Video-LLaMA" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b class="best-score-text">42.68</b></td>
              <td>50.3</td>
              <td>29.7</td>
              <td>40.9</td>
              <td>49.3</td>
              <td>43.3</td>
              <td>33.9</td>                          
            </tr>
            <tr>
              <td>2</td>
              <td><b class="best-score-text">Chat-UniVi</b></td>
              <td>Sequential</td>
              <td><a href="https://github.com/PKU-YuanGroup/Chat-UniVi" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b class="best-score-text">42.68</b></td>
              <td>50.3</td>
              <td>29.7</td>
              <td>40.9</td>
              <td>49.3</td>
              <td>43.3</td>
              <td>33.9</td>                          
            </tr>
            <tr>
              <td>2</td>
              <td><b class="best-score-text">Chat-UniVi</b></td>
              <td>Combined</td>
              <td><a href="https://github.com/PKU-YuanGroup/Chat-UniVi" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b class="best-score-text">42.68</b></td>
              <td>50.3</td>
              <td>29.7</td>
              <td>40.9</td>
              <td>49.3</td>
              <td>43.3</td>
              <td>33.9</td>                          
            </tr>
            <tr>
              <td>5</td>
              <td><b>LLaVA-1.5</b></td>
              <td>Combined</td>
              <td><a href="https://github.com/haotian-liu/LLaVA" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b>25.40</b></td>
              <td>22.9</td>
              <td>24.6</td>
              <td>18.1</td>
              <td>35.8</td>
              <td>29.7</td>
              <td>26.9</td>                              
            </tr>       
            <tr>
              <td>5</td>
              <td><b>MiniGPT4</b></td>
              <td>Combined</td>
              <td><a href="https://github.com/Vision-CAIR/MiniGPT-4" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b>25.40</b></td>
              <td>22.9</td>
              <td>24.6</td>
              <td>18.1</td>
              <td>35.8</td>
              <td>29.7</td>
              <td>26.9</td>                              
            </tr>    
            <tr>
              <td>5</td>
              <td><b>MiniGPT5</b></td>
              <td>Combined</td>
              <td><a href="https://github.com/eric-ai-lab/MiniGPT-5" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b>25.40</b></td>
              <td>22.9</td>
              <td>24.6</td>
              <td>18.1</td>
              <td>35.8</td>
              <td>29.7</td>
              <td>26.9</td>                              
            </tr>   
            <tr>
              <td>5</td>
              <td><b>mPLUG_Owl-v2</b></td>
              <td>Combined</td>
              <td><a href="https://github.com/X-PLUG/mPLUG-Owl" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b>25.40</b></td>
              <td>22.9</td>
              <td>24.6</td>
              <td>18.1</td>
              <td>35.8</td>
              <td>29.7</td>
              <td>26.9</td>                              
            </tr> 
            <tr>
              <td>5</td>
              <td><b>InstructBLIP</b></td>
              <td>Combined</td>
              <td><a href="https://github.com/salesforce/LAVIS/tree/main/projects/instructblip" class="ext-link" style="font-size: 16px;">Link</a></td>
              <td>2024-01-20</td>
              <td><b>25.40</b></td>
              <td>22.9</td>
              <td>24.6</td>
              <td>18.1</td>
              <td>35.8</td>
              <td>29.7</td>
              <td>26.9</td>                              
            </tr> 
        </table>
        </div>
  
      </div>
    </div>
  
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            <img src="static/images/film_frames.png" style="hight:2.0em;vertical-align: middle" alt="Logo"/>
            <span>Mementos</span> is a consolidated Mathematical reasoning benchmark within
            Visual contexts. It consists of <b>three newly created datasets, IQTest, FunctionQA, and PaperQA</b>, which address the missing visual domains and are tailored to evaluate logical reasoning on puzzle test figures,
            algebraic reasoning over functional plots, and scientific reasoning with academic paper figures, respectively. It also incorporates <b>9 MathQA datasets</b>
            and <b>19 VQA datasets</b> from the literature, which significantly enrich the diversity and complexity of
            visual perception and mathematical reasoning challenges within our benchmark. 
            In total, <img src="static/images/mathvista.png" style="hight:2.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">MathVista</span> includes <b>6,141 examples</b> collected from <b>31 different datasets</b>.
            <!-- a compilation of data 1) carefully examined and filtered from 28 existing VQA and MathQA datasets and 2) manually collected by us. In total, 6,141 examples were collected from 31 different datasets. -->
          </p>

          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/demo.png" alt="algebraic reasoning" width="80%"/>
                <p> Examples of our newly annotated datasets: IQTest, FunctionQA, and PaperQA.</p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/main_results_tab.png" alt="arithmetic reasoning" width="50%"/>
                <p> Summary of the 31 different source datasets in <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                  <span class="mathvista">MathVista</span>.
              </div>
            </div>
          </div>
        </div>
        </div>
      </div>
    </div>
  </section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{wang2024mementos,
      title={Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences}, 
      author={Xiyao Wang and Yuhang Zhou and Xiaoyu Liu and Hongjin Lu and Yuancheng Xu and Feihong He and Jaehong Yoon and Taixi Lu and Gedas Bertasius and Mohit Bansal and Huaxiu Yao and Furong Huang},
      year={2024},
      eprint={2401.10529},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
