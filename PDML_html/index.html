<!DOCTYPE html>
<html lang="en">

  <head>

  </head>
<head>
  <!-- Title -->
  <title>Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy</title>

  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy">
  <meta name="keywords" content="dynamics model learning model-based reinforcement learning, distribution shift, replay buffer">

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <!-- https://fontawesome.com/cheatsheet -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124898353-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-124898353-1');
  </script>


</head>


<body>
  <!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> -->
  <nav class="navbar navbar-expand-md fixed-top navbar-dark" style="background-color: #1631c7;">
    <a class="navbar-brand" href="#">Live in the Moment</a>

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarToggle">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="#">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Abstract">Abstract</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Motivation">Motivation</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Method">Method</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Results">Results</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Cite">Cite</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container" style="padding-top: 80px; font-size: 20px">
    <div align="center">
      <h2 class="text-center" align="center">
        Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy
      </h2><br>
      <h6>
      <!--  <a href="https://people.csail.mit.edu/jiex">Jie Xu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;  -->
      <a href="https://si0wang.github.io/">Xiyao Wang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
      <a href="https://wwongkamjan.github.io/">Wichayaporn Wongkamjan</a><sup>1</sup>&nbsp;&nbsp;
      Ruonan Jia</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://furong-huang.com/">Furong Huang</a><sup>1</sup>
      </h6>
      <small><sup>1</sup> University of Maryland, College Park &nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup>  Tsinghua University
</small>
        <i><font color="black"> </font></i><br>
        <a target="_blank" href="https://proceedings.mlr.press/v202/wang23an.html">[Paper]</a>&nbsp;
        <a target="_blank" href="https://github.com/umd-huang-lab/PDML">[Code]</a>&nbsp;
    </div>
  </div><br>
  <!-- Abstract -->
  <div class="container">
    <h3 id="Abstract" style="padding-top: 70px; margin-top: -80px; ">Abstract</h3><hr>
    <div style="text-align: justify">
      Model-based reinforcement learning (RL) often achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a dynamics model that fits under the empirical state-action visitation distribution for all historical policies, i.e., the sample replay buffer. However, in this paper, we observe that fitting the dynamics model under the distribution for all his- torical policies does not necessarily benefit model prediction for the current policy since the policy in use is constantly evolving over time. The evolv- ing policy during training will cause state-action visitation distribution shifts. We theoretically an- alyze how this distribution shift over historical policies affects the model learning and model roll- outs. We then propose a novel dynamics model learning method, named Policy-adapted Dynam- ics Model Learning (PDML). PDML dynamically adjusts the historical policy mixture distribution to ensure the learned model can continually adapt to the state-action visitation distribution of the evolving policy. Experiments on a range of contin- uous control environments in MuJoCo show that PDML achieves significant improvement in sam- ple efficiency and higher asymptotic performance combined with the state-of-the-art model-based RL methods.
    </div>
  </div><br><br>

  <!-- Paper -->

  <!-- Method: Breifly Describe our algorithm -->
  <div class="container">
    <h3 id="Motivation" style="padding-top: 70px; margin-top: -80px; ">Mismatch between model learning and model rollouts</h3><hr>
    <div style="text-align: justify">
    <h4>State-action visitation distribution shift during policy learning</h4>:
    The state-action visitation distribution of policies under different environment steps is very different.
    <b> HalfCheetah </b>:
    <img class="img-responsive img-rounded" src="./Figures/policy_distribution_shift_halfcheetah.png" style="width:100%; height:100%" alt="">
    <b> Hopper </b>:
    <img class="img-responsive img-rounded" src="./Figures/policy_distribution_shift_hopper.png" style="width:100%; height:100%" alt="">
    <h4>Prediction error curves of MBPO</h4>:
    Overall error means the model prediction error for all historical policies, and current error is the model prediction error for the current policy. We observe that there is a gap between the overall error and the current error. This means although the agent can learn a dynamics model which is good enough for all samples obtained by historical poli- cies, this is at the expense of the prediction accuracy for the samples induced by current policy.
    <img class="img-responsive img-rounded" src="./Figures/error_curve.png" style="width:100%; height:100%" alt="">
    </div>
  </div><br><br>

  <!-- Experiments: show our experimental results -->
  <div class="container">
    <h3 id="Method" style="padding-top: 70px; margin-top: -80px; ">Method</h3><hr>
    <div style="text-align: justify">
      <h4>Policy-adapted Dynamics Model Learning</h4><hr>
      Our main idea is to adjust the policy mixture distribution that continuously adapts to the current policy.
      <b> Weights designed for historical policies</b>:
      <img class="img-responsive img-rounded" src="./Figures/historical_policy_weight.png" style="width:80%; height:80%" alt=""></center>
      <b> Weights designed for current policy</b>:
      <img class="img-responsive img-rounded" src="./Figures/current_policy_weight.png" style="width:80%; height:80%" alt=""></center>
      <b> Estimation of the policy distribution shift</b>:
      <img class="img-responsive img-rounded" src="./Figures/estimation_distribution_shift.png" style="width:80%; height:80%" alt=""></center>
    </div>
  </div><br><br>


  <div class="container">
    <h4 id="Cite" style="padding-top: 70px; margin-top: -80px;">Cite This Paper</h4><hr>
    <div class="row">
      <div class="col-md-12">
        @inproceedings{wang2023live,
      title={Live in the moment: Learning dynamics model adapted to evolving policy},
      author={Wang, Xiyao and Wongkamjan, Wichayaporn and Jia, Ruonan and Huang, Furong},
      booktitle={International Conference on Machine Learning},
      pages={36470--36493},
      year={2023},
      organization={PMLR}
    }<br>
      </div>
    </div>
  </div> <br>

  <!-- Footer -->
  <div class="container col-md-9">
    <hr>
    <center>
      <footer>
        <p>Â© University of Maryland, College Park 2023</p>
      </footer>
    </center>
  </div>


  <!-- Bootstrap core JavaScript -->
  <!-- Placed at the end of the document so the pages load faster -->
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>

</html>
